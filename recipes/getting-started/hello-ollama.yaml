# Hello Ollama â€” Minimal getting-started recipe
# Tests that Ollama is working and demonstrates basic variable resolution.
# The LLM response is printed in the workflow log output.

name: "hello-ollama"
version: "1.0"
description: "Generate text with a local Ollama model"

inputs:
  - name: "prompt"
    type: "string"
    required: true
    description: "What to ask the local LLM"

  - name: "model"
    type: "string"
    default: "llama3.2:3b"
    description: "Ollama model to use"

steps:
  - id: "generate"
    name: "Generate with Ollama"
    tool: "ollama"
    action: "generate"
    inputs:
      model: "{{inputs.model}}"
      prompt: "{{inputs.prompt}}"

  - id: "setup_output"
    name: "Create output directory"
    tool: "file_ops"
    action: "mkdir"
    inputs:
      path: "{{temp_dir}}/output"

cleanup:
  on_success:
    - action: "preserve"
      path: "{{temp_dir}}"
      reason: "output"

# Usage:
# python -m localforge run recipes/getting-started/hello-ollama.yaml \
#   --input prompt="Tell me a joke" --auto-approve
#
# The LLM response is available at: steps.generate.outputs.response
# It's printed in the workflow log and preserved in the run directory.
